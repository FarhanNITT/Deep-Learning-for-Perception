{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317efffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import nbimporter\n",
    "import unittest\n",
    "\n",
    "import torch.nn.functional as f\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9f4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil = Image.open('main.jpg').convert('L')    # converting rgb image to grayscale image\n",
    "image = np.array(image_pil)                       # converted image into numpy array\n",
    "\n",
    "KERNEL = np.array(\n",
    "    [\n",
    "        [1, 0, -1], \n",
    "        [2, 0, -2],\n",
    "        [1, 0, -1]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b722b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_scipy_convolve2d(img, kernel):\n",
    "    start =time.time()\n",
    "    filtered_image = np.zeros_like(img).astype(np.float64)   # filtered image variable array of size same as input image\n",
    "    \n",
    "    #casted into double data type using float 64\n",
    "    \n",
    "    filtered_image = scipy.signal.convolve2d(img, kernel, mode='same')\n",
    "\n",
    "    print(\"Elapsed time (s)=\", time.time() - start)\n",
    "    return filtered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de045dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time (s)= 7.865091800689697\n"
     ]
    }
   ],
   "source": [
    "img = filter_scipy_convolve2d(image, KERNEL)\n",
    "writeDoubleImage(img, \"scipy.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53b1bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1.2\n",
    "def filter_numpy_for_loop(img, kernel):\n",
    "    start = time.time()\n",
    "    filtered_image = np.zeros_like(img).astype(np.float64)\n",
    "    # flip my kernel for better alignment with image\n",
    "    \n",
    "\n",
    "    # image and kernel dimesnions to run for loop iteration for convolution\n",
    "   \n",
    "\n",
    "    # estimating padding size \n",
    "    # pad_h = k_h // 2\n",
    "    # pad_w = k_w // 2\n",
    "\n",
    "    #padded_image = np.pad(img,pad_width=pad_size,mode='constant',constant_values=0)\n",
    "    padded_image = np.pad(img, pad_width =1, mode='constant', constant_values=0)\n",
    "    K_flip = np.flip(kernel)\n",
    "    \n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "\n",
    "            # what part of image will be convoluted\n",
    "            # convolute = padded_image[i:i+k_h,j:j+k_w]\n",
    "            # apply convolution - step 1 - element wise multiplication of kernel and convolute area ; step 2: summation\n",
    "            #prod = convolute * K_flip  # step 1\n",
    "\n",
    "            filtered_image[i,j] = np.sum(padded_image[i:i+3,j:j+3] * K_flip)  # step 2\n",
    "\n",
    "    print(\"Elapsed time (s)=\", time.time() - start)\n",
    "    return filtered_image\n",
    "\n",
    "# Part 1.2\n",
    "# def filter_numpy_for_loop(img, kernel):\n",
    "#     start = time.time()\n",
    "#     filtered_image = np.zeros_like(img).astype(np.float64)\n",
    "#     # flip my kernel for better alignment with image\n",
    "    \n",
    "\n",
    "#     # image and kernel dimesnions to run for loop iteration for convolution\n",
    "   \n",
    "\n",
    "    # estimating padding size \n",
    "    # pad_h = k_h // 2\n",
    "    # pad_w = k_w // 2\n",
    "\n",
    "    #padded_image = np.pad(img,pad_width=pad_size,mode='constant',constant_values=0)\n",
    "    # padded_image = np.pad(img, pad_width =1, mode='constant', constant_values=0)\n",
    "    # K_flip = np.flip(kernel)\n",
    "    \n",
    "    # for i in range(img.shape[0]):\n",
    "    #     for j in range(img.shape[1]):\n",
    "\n",
    "    #         # what part of image will be convoluted\n",
    "    #         # convolute = padded_image[i:i+k_h,j:j+k_w]\n",
    "    #         # apply convolution - step 1 - element wise multiplication of kernel and convolute area ; step 2: summation\n",
    "    #         #prod = convolute * K_flip  # step 1\n",
    "\n",
    "    #         filtered_image[i,j] = np.sum(padded_image[i:i+3,j:j+3] * K_flip)  # step 2\n",
    "\n",
    "    # print(\"Elapsed time (s)=\", time.time() - start)\n",
    "    # return filtered_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7efce8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time (s)= 19.600974321365356\n"
     ]
    }
   ],
   "source": [
    "img1 = filter_numpy_for_loop(image, KERNEL)\n",
    "writeDoubleImage(img1, \"numpy_for_loop.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec329337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1.3\n",
    "\n",
    "def filter_torch(img, kernel):\n",
    "    start = time.time()\n",
    "    flip_kernel = np.flip(kernel).copy()\n",
    "    conv_layer = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1) # One input channel (grayscaled img), one output channel (one kernel), kernel_size =3\n",
    "    weight_tensor = torch.tensor(flip_kernel, dtype=torch.float64)\n",
    "\n",
    "    with torch.no_grad():  # To Hardcode values, this ensures the weights dont update due to gradient tracking\n",
    "      conv_layer.weight.data = weight_tensor.unsqueeze(0).unsqueeze(0) #Incrase dim of tensor to indicate batch size,\n",
    "      conv_layer.bias = torch.nn.Parameter(torch.tensor([0.0], dtype=torch.float64)) #Ensuring bias has the same dimensions as weights\n",
    "    \n",
    "    padded_img = np.pad(img, pad_width=1, mode='constant', constant_values=0)\n",
    "    img_tensor = torch.tensor(padded_img, dtype=torch.float64) # Convert img to tensor with dtype = double\n",
    "    img_input = img_tensor.unsqueeze(0).unsqueeze(0) # Increasing the dimension of the tensor to indicate batch size to pytorch\n",
    "    #filtered_image = conv_layer(torch.tensor(img_input, dtype=torch.float64))\n",
    "    filtered_image = conv_layer(img_input.double()).squeeze()\n",
    "\n",
    "    print(\"Elapsed time (s)=\", time.time() - start)\n",
    "    return filtered_image.detach().numpy()\n",
    "\n",
    "\n",
    "# def filter_torch(img, kernel):\n",
    "#     start = time.time()\n",
    "\n",
    "#     K_flip = np.flip(kernel)  # flipped numpy kernel\n",
    "\n",
    "#     # unsqueeze is used to add dimension to tensor at a given position\n",
    "#     # for conv2d function, we need a dimension for channels and batch dimnsion -- hence repeated twice\n",
    "#     # converted to float to ensure correct data type and easy operation\n",
    "    \n",
    "#     tensor_image = torch.from_numpy(img).float().unsqueeze(0).unsqueeze(0)  # converting image numpy array to tensor\n",
    "#     tensor_kernel = torch.from_numpy(K_flip.copy()).float().unsqueeze(0).unsqueeze(0)  # tensor kernel \n",
    "    \n",
    "# #     tensor_kernel = tensor_kernel.unsqueeze(0)   # input format for conv2d function\n",
    "\n",
    "# #     tensor_image = tensor_image.unsqueeze(0)\n",
    "    \n",
    "#     #print(tensor_image.size())\n",
    "\n",
    "#     filtered_image = np.zeros_like(img).astype(np.float64)\n",
    "#     filtered_image = f.conv2d(tensor_image, tensor_kernel, padding =1).squeeze(0).squeeze(0)  # removing added dimension ensuring output image is of same size as input image\n",
    "    \n",
    "    \n",
    "#     print(\"Elapsed time (s)=\", time.time() - start)\n",
    "   \n",
    "#     return filtered_image.detach().numpy()   # tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e205314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time (s)= 0.029062986373901367\n"
     ]
    }
   ],
   "source": [
    "img2 = filter_torch(image, KERNEL)\n",
    "writeDoubleImage(img2, \"torch_conv.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01dcba1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d21132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1  0  1]\n",
      " [-2  0  2]\n",
      " [-1  0  1]]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
